"""
çµ±åˆç‰ˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ20251024ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰

Day 4ã®IssueExtractorãƒ™ãƒ¼ã‚¹ + ç™ºè¡Œæ ¹æ‹ æ³•ä»¤æŠ½å‡ºæ©Ÿèƒ½
"""

import os
import sys
import re
import json
from datetime import datetime, date
import uuid

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from google.cloud import bigquery
from parsers.issue_extractor import IssueExtractor

# BigQueryè¨­å®š
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r"C:\Users\sonke\secrets\jgb2023-f8c9b849ae2d.json"
PROJECT_ID = 'jgb2023'
DATASET_ID = '20251024'

client = bigquery.Client(project=PROJECT_ID)

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\JGBãƒ‡ãƒ¼ã‚¿\2023"

# ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ãƒ‘ã‚¿ãƒ¼ãƒ³
LEGAL_BASIS_PATTERNS = {
    '4æ¡å‚µ': {
        'category': '4æ¡å‚µ',
        'sub_category': 'å»ºè¨­å›½å‚µ',
        'patterns': [
            r'è²¡æ”¿æ³•.*?ç¬¬4æ¡ç¬¬1é …',
            r'è²¡æ”¿æ³•.*?ç¬¬å››æ¡ç¬¬ä¸€é …'
        ]
    },
    'ç‰¹ä¾‹å‚µ': {
        'category': 'ç‰¹ä¾‹å‚µ',
        'sub_category': 'ç‰¹ä¾‹å…¬å‚µ',
        'patterns': [
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬2æ¡ç¬¬1é …',
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬äºŒæ¡ç¬¬ä¸€é …',
            r'å¹³æˆ24å¹´æ³•å¾‹ç¬¬101å·.*?ç¬¬2æ¡ç¬¬1é …'
        ]
    },
    'GXçµŒæ¸ˆç§»è¡Œå‚µ': {
        'category': 'GXçµŒæ¸ˆç§»è¡Œå‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬7æ¡ç¬¬1é …',
            r'è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬ä¸ƒæ¡ç¬¬ä¸€é …',
            r'ä»¤å’Œ5å¹´æ³•å¾‹ç¬¬32å·.*?ç¬¬7æ¡ç¬¬1é …'
        ]
    },
    'å¾©èˆˆå‚µ': {
        'category': 'å¾©èˆˆå‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'æ±æ—¥æœ¬å¤§éœ‡ç½ã‹ã‚‰ã®å¾©èˆˆã®ãŸã‚ã®æ–½ç­–ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã«é–¢ã™ã‚‹ç‰¹åˆ¥æªç½®æ³•.*?ç¬¬69æ¡ç¬¬4é …',
            r'æ±æ—¥æœ¬å¤§éœ‡ç½ã‹ã‚‰ã®å¾©èˆˆã®ãŸã‚ã®æ–½ç­–ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã«é–¢ã™ã‚‹ç‰¹åˆ¥æªç½®æ³•.*?ç¬¬å…­åä¹æ¡ç¬¬å››é …'
        ]
    },
    'å¹´é‡‘ç‰¹ä¾‹å‚µ': {
        'category': 'å¹´é‡‘ç‰¹ä¾‹å‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬3æ¡ç¬¬1é …',
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬ä¸‰æ¡ç¬¬ä¸€é …'
        ]
    },
    'å€Ÿæ›å‚µ': {
        'category': 'å€Ÿæ›å‚µ',
        'sub_category': 'å€Ÿæ›å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬46æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å››åå…­æ¡ç¬¬ä¸€é …'
        ]
    },
    'å‰å€’å‚µ': {
        'category': 'å€Ÿæ›å‚µ',
        'sub_category': 'å‰å€’å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬47æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å››åä¸ƒæ¡ç¬¬ä¸€é …'
        ]
    },
    'è²¡æŠ•å‚µ': {
        'category': 'è²¡æŠ•å‚µ',
        'sub_category': 'è²¡æŠ•å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬62æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å…­åäºŒæ¡ç¬¬ä¸€é …'
        ]
    }
}

# æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
SEIFU_TANKI_PATTERNS = [
    r'è²¡æ”¿æ³•.*?ç¬¬7æ¡ç¬¬1é …',
    r'è²¡æ”¿èè³‡è³‡é‡‘æ³•.*?ç¬¬9æ¡ç¬¬1é …',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬83æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬94æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬95æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬136æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬137æ¡'
]


def extract_legal_bases(text):
    """ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å¯¾å¿œã€é‡‘é¡ã‚‚æŠ½å‡ºï¼‰"""
    results = []
    
    for basis_name, config in LEGAL_BASIS_PATTERNS.items():
        for pattern in config['patterns']:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                full_text = match.group(0)
                amount = extract_amount_for_legal_basis(text, full_text)
                
                results.append({
                    'basis': basis_name,
                    'category': config['category'],
                    'sub_category': config['sub_category'],
                    'full': full_text,
                    'amount': amount
                })
                break
    
    return results


def extract_amount_for_legal_basis(text, legal_text):
    """ç‰¹å®šã®ç™ºè¡Œæ ¹æ‹ ã«ç´ã¥ãé‡‘é¡ã‚’æŠ½å‡º"""
    pattern1 = re.escape(legal_text) + r'[^å††]*?é¡é¢é‡‘é¡ã§([\d,]+)å††'
    match = re.search(pattern1, text, re.IGNORECASE | re.DOTALL)
    
    if match:
        return int(match.group(1).replace(',', ''))
    
    pattern2 = re.escape(legal_text) + r'[^å††]*?é‡‘é¡([\d,]+)å††'
    match = re.search(pattern2, text, re.IGNORECASE | re.DOTALL)
    
    if match:
        return int(match.group(1).replace(',', ''))
    
    return None


def has_waribiki_tanki(text):
    """å‰²å¼•çŸ­æœŸå›½å‚µãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    return 'å‰²å¼•çŸ­æœŸå›½å‚µ' in text


def has_government_short_term_bond(text):
    """æ”¿åºœçŸ­æœŸè¨¼åˆ¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    has_seifu_pattern = any(re.search(p, text, re.IGNORECASE) for p in SEIFU_TANKI_PATTERNS)
    has_seifu_word = 'æ”¿åºœçŸ­æœŸè¨¼åˆ¸' in text
    return has_seifu_pattern or has_seifu_word


def extract_series_number(text):
    """å›å·ã‚’æŠ½å‡º"""
    match = re.search(r'ç¬¬(\d+)å›', text)
    if match:
        return f"ç¬¬{match.group(1)}å›"
    return None


def calculate_maturity_period(issuance_date, maturity_date):
    """å„Ÿé‚„æœŸé–“ã‚’è¨ˆç®—"""
    if not issuance_date or not maturity_date:
        return None, None, None
    
    if isinstance(issuance_date, str):
        issuance_date = datetime.strptime(issuance_date, '%Y-%m-%d').date()
    if isinstance(maturity_date, str):
        maturity_date = datetime.strptime(maturity_date, '%Y-%m-%d').date()
    
    delta = maturity_date - issuance_date
    days = delta.days
    months = days / 30.0
    years = days / 365.0
    
    return days, months, years


def determine_bond_master_id(bond_type, maturity_days):
    """å‚µåˆ¸ç¨®é¡ã¨å„Ÿé‚„æœŸé–“ã‹ã‚‰bond_master_idã‚’æ±ºå®š"""
    
    # çŸ­æœŸè¨¼åˆ¸
    if bond_type == 'çŸ­æœŸè¨¼åˆ¸' or bond_type == 'çŸ­æœŸå¹´':
        if maturity_days and maturity_days >= 270:
            return 'BOND_TANKI_1Y'
        else:
            return 'BOND_TANKI_6M'
    
    # GXå‚µ
    if 'GX' in bond_type or 'ã‚¯ãƒ©ã‚¤ãƒ¡ãƒ¼ãƒˆ' in bond_type:
        if '10å¹´' in bond_type or (maturity_days and 3000 <= maturity_days <= 4000):
            return 'BOND_GX_10Y'
        elif '5å¹´' in bond_type or (maturity_days and 1500 <= maturity_days <= 2000):
            return 'BOND_GX_5Y'
    
    # é€šå¸¸ã®åˆ©ä»˜å‚µ
    mapping = {
        '2å¹´': 'BOND_002Y',
        '5å¹´': 'BOND_005Y',
        '10å¹´': 'BOND_010Y',
        '20å¹´': 'BOND_020Y',
        '30å¹´': 'BOND_030Y',
        '40å¹´': 'BOND_040Y'
    }
    
    for key, value in mapping.items():
        if key in bond_type:
            return value
    
    # å€‹äººå‘ã‘å›½å‚µ
    if 'å€‹äººå‘ã‘' in bond_type or 'å›ºå®š' in bond_type or 'å¤‰å‹•' in bond_type:
        if 'å›ºå®š' in bond_type and '3å¹´' in bond_type:
            return 'BOND_KOJIN_FIXED_3Y'
        elif 'å›ºå®š' in bond_type and '5å¹´' in bond_type:
            return 'BOND_KOJIN_FIXED_5Y'
        elif 'å¤‰å‹•' in bond_type and '10å¹´' in bond_type:
            return 'BOND_KOJIN_VARIABLE_10Y'
    
    # ç‰©ä¾¡é€£å‹•
    if 'ç‰©ä¾¡é€£å‹•' in bond_type or 'ç‰©ä¾¡' in bond_type:
        return 'BOND_BUKKA_10Y'
    
    return None


def split_by_legal_basis(issuance_dict, legal_bases):
    """1ã¤ã®ç™ºè¡Œã‚’è¤‡æ•°ã®ç™ºè¡Œæ ¹æ‹ ã«åˆ†é›¢"""
    if len(legal_bases) == 0:
        return [issuance_dict]
    
    if len(legal_bases) == 1:
        issuance_dict['legal_basis'] = legal_bases[0]['basis']
        issuance_dict['legal_basis_full'] = legal_bases[0]['full']
        issuance_dict['legal_basis_category'] = legal_bases[0]['category']
        return [issuance_dict]
    
    # è¤‡æ•°ã®ç™ºè¡Œæ ¹æ‹ ãŒã‚ã‚‹å ´åˆã¯åˆ†é›¢
    split_issuances = []
    total_extracted_amount = sum([lb['amount'] for lb in legal_bases if lb['amount']])
    original_amount = issuance_dict['issue_amount']
    
    for lb in legal_bases:
        new_issuance = issuance_dict.copy()
        new_issuance['issuance_id'] = str(uuid.uuid4())
        new_issuance['legal_basis'] = lb['basis']
        new_issuance['legal_basis_full'] = lb['full']
        new_issuance['legal_basis_category'] = lb['category']
        new_issuance['is_split'] = True
        new_issuance['parent_issuance_id'] = issuance_dict['issuance_id']
        new_issuance['split_reason'] = 'è¤‡æ•°ç™ºè¡Œæ ¹æ‹ '
        
        if lb['amount']:
            new_issuance['issue_amount'] = lb['amount']
        else:
            unassigned_count = sum(1 for x in legal_bases if not x['amount'])
            if unassigned_count > 0 and original_amount:
                remaining = original_amount - total_extracted_amount
                new_issuance['issue_amount'] = remaining // unassigned_count
        
        split_issuances.append(new_issuance)
    
    return split_issuances


def process_announcement_file(file_path):
    """1ã¤ã®å‘Šç¤ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆIssueExtractorä½¿ç”¨ï¼‰"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    file_name = os.path.basename(file_path)
    announcement_id = file_name.replace('.txt', '')
    
    # å‰²å¼•çŸ­æœŸå›½å‚µã¨æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®åˆ¤å®š
    has_waribiki = has_waribiki_tanki(content)
    has_seifu = has_government_short_term_bond(content)
    
    # æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if has_seifu and not has_waribiki:
        print(f"  âš ï¸  æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return None
    
    if has_waribiki and has_seifu:
        print(f"  ğŸ“ å‰²å¼•çŸ­æœŸå›½å‚µ+æ”¿åºœçŸ­æœŸè¨¼åˆ¸ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µã®ã¿æŠ½å‡ºï¼‰")
    
    # å‘Šç¤ºæ—¥ã‚’æŠ½å‡º
    date_match = re.match(r'(\d{8})_', file_name)
    if date_match:
        date_str = date_match.group(1)
        kanpo_date = datetime.strptime(date_str, '%Y%m%d').date()
    else:
        kanpo_date = None
    
    announcement_number = extract_series_number(content)
    
    announcement_data = {
        'announcement_id': announcement_id,
        'kanpo_date': kanpo_date.isoformat() if kanpo_date else None,
        'announcement_number': announcement_number,
        'source_file': file_name,
        'created_at': datetime.now().isoformat(),
        'updated_at': datetime.now().isoformat()
    }
    
    # ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ã‚’æŠ½å‡º
    legal_bases = extract_legal_bases(content)
    
    # æ”¿åºœçŸ­æœŸè¨¼åˆ¸é–¢é€£ã‚’é™¤å¤–
    if legal_bases:
        legal_bases = [lb for lb in legal_bases if lb['basis'] in 
                      ['å€Ÿæ›å‚µ', 'å‰å€’å‚µ', '4æ¡å‚µ', 'ç‰¹ä¾‹å‚µ', 'GXçµŒæ¸ˆç§»è¡Œå‚µ', 'å¾©èˆˆå‚µ', 'å¹´é‡‘ç‰¹ä¾‹å‚µ']]
    
    if legal_bases:
        basis_summary = ', '.join([f"{lb['basis']}({lb['category']})" for lb in legal_bases])
        print(f"  â†’ ç™ºè¡Œæ ¹æ‹ : {basis_summary}")
    
    # IssueExtractorã§ç™ºè¡Œæƒ…å ±ã‚’æŠ½å‡º
    extractor = IssueExtractor()
    issuances = extractor.extract_issues(content)
    
    print(f"  â†’ æŠ½å‡ºä»¶æ•°: {len(issuances)}ä»¶")
    
    # å„ç™ºè¡Œæƒ…å ±ã‚’å‡¦ç†
    all_issuances = []
    for issuance in issuances:
        # æœŸé–“ã‚’è¨ˆç®—
        days, months, years = calculate_maturity_period(
            issuance.issuance_date,
            issuance.maturity_date
        )
        
        # bond_master_idã‚’æ±ºå®š
        bond_master_id = determine_bond_master_id(issuance.bond_type, days)
        
        # æ—¥ä»˜ã‚’æ–‡å­—åˆ—åŒ–
        issuance_date_str = issuance.issuance_date.isoformat() if isinstance(issuance.issuance_date, date) else issuance.issuance_date
        maturity_date_str = issuance.maturity_date.isoformat() if isinstance(issuance.maturity_date, date) else issuance.maturity_date
        payment_date_str = issuance.payment_date.isoformat() if isinstance(issuance.payment_date, date) else issuance.payment_date
        
        issuance_dict = {
            'issuance_id': str(uuid.uuid4()),
            'announcement_id': announcement_id,
            'bond_master_id': bond_master_id,
            'series_number': issuance.series_number or announcement_number,
            'issuance_date': issuance_date_str,
            'maturity_date': maturity_date_str,
            'payment_date': payment_date_str,
            'maturity_period_days': days,
            'maturity_period_months': months,
            'maturity_period_years': years,
            'issue_amount': issuance.face_value_individual,
            'issue_price': issuance.issue_price,
            'interest_rate': issuance.interest_rate,
            'legal_basis': None,
            'legal_basis_full': None,
            'legal_basis_category': None,
            'issue_method': None,
            'is_split': False,
            'parent_issuance_id': None,
            'split_reason': None,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # ç™ºè¡Œæ ¹æ‹ ã«ã‚ˆã‚‹åˆ†é›¢
        split_issuances = split_by_legal_basis(issuance_dict, legal_bases)
        all_issuances.extend(split_issuances)
    
    return {
        'announcement': announcement_data,
        'issuances': all_issuances
    }


def upload_to_bigquery(data_list):
    """BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    # announcements
    announcements = [d['announcement'] for d in data_list if d]
    if announcements:
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.announcements"
        errors = client.insert_rows_json(table_ref, announcements)
        if errors:
            print(f"âŒ announcementsæŠ•å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
        else:
            print(f"âœ… announcementsæŠ•å…¥æˆåŠŸ: {len(announcements)}ä»¶")
    
    # bond_issuances
    all_issuances = []
    for d in data_list:
        if d and d['issuances']:
            all_issuances.extend(d['issuances'])
    
    if all_issuances:
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.bond_issuances"
        errors = client.insert_rows_json(table_ref, all_issuances)
        if errors:
            print(f"âŒ bond_issuancesæŠ•å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
        else:
            print(f"âœ… bond_issuancesæŠ•å…¥æˆåŠŸ: {len(all_issuances)}ä»¶")
    
    return len(all_issuances)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("çµ±åˆç‰ˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ20251024ï¼‰")
    print("Day 4 IssueExtractor + ç™ºè¡Œæ ¹æ‹ æ³•ä»¤æŠ½å‡º")
    print("=" * 60)
    print()
    
    files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.txt')])
    print(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}ä»¶\n")
    
    data_list = []
    skip_count = 0
    mixed_count = 0
    error_count = 0
    
    for i, file_name in enumerate(files, 1):
        file_path = os.path.join(DATA_DIR, file_name)
        print(f"[{i}/{len(files)}] {file_name}")
        
        try:
            result = process_announcement_file(file_path)
            if result:
                data_list.append(result)
                print(f"  âœ… ç™ºè¡Œä»¶æ•°: {len(result['issuances'])}ä»¶")
                
                # æ··åœ¨å‘Šç¤ºã®ã‚«ã‚¦ãƒ³ãƒˆ
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if has_waribiki_tanki(content) and has_government_short_term_bond(content):
                        mixed_count += 1
            else:
                skip_count += 1
        except Exception as e:
            error_count += 1
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n=== å‡¦ç†å®Œäº† ===")
    print(f"æˆåŠŸ: {len(data_list)}ä»¶")
    print(f"  ã†ã¡æ··åœ¨å‘Šç¤º: {mixed_count}ä»¶ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µ+æ”¿åºœçŸ­æœŸè¨¼åˆ¸ï¼‰")
    print(f"ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶ï¼ˆæ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ï¼‰")
    print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    
    if data_list:
        print("\n=== BigQueryæŠ•å…¥é–‹å§‹ ===")
        total_issuances = upload_to_bigquery(data_list)
        print(f"\nç·ç™ºè¡Œä»¶æ•°: {total_issuances}ä»¶")
        
        # ç™ºè¡Œæ ¹æ‹ åˆ¥é›†è¨ˆ
        print("\n=== ç™ºè¡Œæ ¹æ‹ åˆ¥é›†è¨ˆ ===")
        legal_basis_summary = {}
        for d in data_list:
            if d and d['issuances']:
                for issuance in d['issuances']:
                    category = issuance.get('legal_basis_category')
                    if category:
                        if category not in legal_basis_summary:
                            legal_basis_summary[category] = {'count': 0, 'amount': 0}
                        legal_basis_summary[category]['count'] += 1
                        legal_basis_summary[category]['amount'] += issuance.get('issue_amount', 0) or 0
        
        if legal_basis_summary:
            for category in sorted(legal_basis_summary.keys()):
                data = legal_basis_summary[category]
                print(f"{category}: {data['count']}ä»¶, {data['amount'] / 100000000:,.0f}å„„å††")
        else:
            print("ï¼ˆç™ºè¡Œæ ¹æ‹ æ³•ä»¤ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")


if __name__ == "__main__":
    main()
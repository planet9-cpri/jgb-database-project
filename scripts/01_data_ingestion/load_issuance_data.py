"""
ç™ºè¡Œãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python scripts/load_issuance_data.py --limit 10
    python scripts/load_issuance_data.py  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from google.cloud import bigquery
import pandas as pd
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from parsers.kanpo_parser import KanpoParser
from parsers.table_parser import TableParser

# è¨­å®š
PROJECT_ID = "jgb2023"
DATASET_ID = "20251019"
DATA_DIR = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\JGBãƒ‡ãƒ¼ã‚¿\2023"
SERVICE_ACCOUNT_KEY = r"C:\Users\sonke\secrets\jgb2023-f8c9b849ae2d.json"


class IssuanceDataLoader:
    """ç™ºè¡Œãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id: str, dataset_id: str, service_account_key: str):
        """åˆæœŸåŒ–"""
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = service_account_key
        self.client = bigquery.Client(project=project_id)
        self.dataset_id = dataset_id
        self.kanpo_parser = KanpoParser()
        self.table_parser = TableParser()
        
        self.stats = {
            'files_processed': 0,
            'files_failed': 0,
            'announcements_inserted': 0,
            'issuances_inserted': 0,
            'legal_basis_inserted': 0,
            'errors': []
        }
    
    def get_kanpo_files(self, data_dir: str, limit: Optional[int] = None) -> List[Path]:
        """å®˜å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’å–å¾—"""
        data_path = Path(data_dir)
        if not data_path.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_dir}")
        
        files = sorted(data_path.glob("*.txt"))
        if limit:
            files = files[:limit]
        
        print(f"ğŸ“‚ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        return files
    
    def parse_kanpo_file(self, file_path: Path) -> Optional[Dict]:
        """å®˜å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        try:
            result = self.kanpo_parser.parse_file(str(file_path))
            if not result:
                print(f"âš ï¸ è§£æå¤±æ•—: {file_path.name}")
                return None
            
            announcement_info = result.get('announcement_info', {})
            
            if not announcement_info.get('kanpo_date'):
                announcement_info = self._extract_info_from_filename(file_path.name)
                print(f"  â„¹ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å‘Šç¤ºæƒ…å ±ã‚’æŠ½å‡º")
            
            tables = result.get('tables', [])
            
            print(f"\nğŸ“„ {file_path.name}")
            print(f"  å‘Šç¤ºæ—¥: {announcement_info.get('kanpo_date', 'ãªã—')}")
            print(f"  å‘Šç¤ºç•ªå·: {announcement_info.get('announcement_number', 'ãªã—')}")
            print(f"  åˆ¥è¡¨æ•°: {len(tables)}")
            
            bond_issuances = []
            
            if tables:
                for table in tables:
                    table_content = table.get('content', '')
                    common_legal_basis = table.get('common_legal_basis')
                    issuances = self.table_parser.parse_table(table_content, common_legal_basis)
                    bond_issuances.extend(issuances)
            else:
                single_issuance = self.table_parser.extract_bond_info_from_single(result.get('content', ''))
                if single_issuance:
                    bond_issuances.append(single_issuance)
            
            print(f"  éŠ˜æŸ„æ•°: {len(bond_issuances)}")
            
            return {
                'announcement': announcement_info,
                'issuances': bond_issuances,
                'source_file': file_path.name
            }
            
        except Exception as e:
            print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {file_path.name} - {e}")
            self.stats['errors'].append({'file': file_path.name, 'error': str(e)})
            return None
    
    def _extract_info_from_filename(self, filename: str) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å‘Šç¤ºæƒ…å ±ã‚’æŠ½å‡º"""
        info = {}
        
        date_match = re.match(r'(\d{8})_', filename)
        if date_match:
            date_str = date_match.group(1)
            info['kanpo_date'] = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        
        number_match = re.search(r'è²¡å‹™çœç¬¬(.+?)å·', filename)
        if number_match:
            number_str = number_match.group(1)
            number = self._convert_kanji_to_number(number_str)
            info['announcement_number'] = f"ç¬¬{number}å·"
        
        gazette_date_match = re.search(r'(ä»¤å’Œ\d+å¹´\d+æœˆ\d+æ—¥ä»˜)', filename)
        if gazette_date_match:
            info['gazette_issue_number'] = gazette_date_match.group(1)
        
        info['title'] = ''
        info['announcement_type'] = 'ç™ºè¡Œå‘Šç¤º'
        
        return info
    
    def _convert_kanji_to_number(self, kanji_str: str) -> str:
        """æ¼¢æ•°å­—ã‚’æ•°å­—ã«å¤‰æ›"""
        kanji_to_digit = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
        
        total = 0
        current = 0
        
        i = 0
        while i < len(kanji_str):
            char = kanji_str[i]
            
            if char in kanji_to_digit:
                current = kanji_to_digit[char]
            elif char == 'å':
                current = 10 if current == 0 else current * 10
            elif char == 'ç™¾':
                current = 100 if current == 0 else current * 100
            elif char == 'åƒ':
                current = 1000 if current == 0 else current * 1000
            elif char == 'ä¸‡':
                current = 10000 if current == 0 else current * 10000
            else:
                i += 1
                continue
            
            if i + 1 < len(kanji_str):
                next_char = kanji_str[i + 1]
                if next_char not in ['å', 'ç™¾', 'åƒ', 'ä¸‡']:
                    total += current
                    current = 0
            else:
                total += current
                current = 0
            
            i += 1
        
        if current > 0:
            total += current
        
        return str(total) if total > 0 else kanji_str
    
    def _convert_wareki_to_date(self, wareki_str: str) -> Optional[str]:
        """å’Œæš¦ã‚’è¥¿æš¦ï¼ˆYYYY-MM-DDï¼‰ã«å¤‰æ›"""
        if not wareki_str or wareki_str == "ä¸æ˜":
            return None
        
        # ä»¤å’Œ
        match = re.match(r'ä»¤å’Œ(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', wareki_str)
        if match:
            reiwa_year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            year = 2018 + reiwa_year  # ä»¤å’Œå…ƒå¹´ = 2019å¹´
            return f"{year:04d}-{month:02d}-{day:02d}"
        
        # å¹³æˆ
        match = re.match(r'å¹³æˆ(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', wareki_str)
        if match:
            heisei_year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            year = 1988 + heisei_year  # å¹³æˆå…ƒå¹´ = 1989å¹´
            return f"{year:04d}-{month:02d}-{day:02d}"
        
        return None
    
    def prepare_announcement_data(self, parsed_data: Dict) -> Dict:
        """å‘Šç¤ºãƒ‡ãƒ¼ã‚¿ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        announcement = parsed_data['announcement']
        
        return {
            'announcement_id': self._generate_announcement_id(announcement),
            'kanpo_date': announcement.get('kanpo_date'),
            'announcement_number': str(announcement.get('announcement_number', 'ä¸æ˜')),
            'gazette_issue_number': str(announcement.get('gazette_issue_number', '')),
            'announcement_type': announcement.get('announcement_type', 'ç™ºè¡Œå‘Šç¤º'),
            'title': announcement.get('title', ''),
            'source_file': parsed_data['source_file'],
            'created_at': datetime.utcnow(),
            'updated_at': datetime.utcnow()
        }
    
    def prepare_issuance_data(self, announcement_id: str, issuances: List) -> List[Dict]:
        """éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        result = []
        
        for idx, issuance in enumerate(issuances):
            issuance_id = f"{announcement_id}_ISSUE_{idx+1:03d}"
            
            # å’Œæš¦ã‹ã‚‰è¥¿æš¦ã«å¤‰æ›
            issuance_date_str = self._convert_wareki_to_date(
                getattr(issuance, 'issuance_date', None)
            )
            maturity_date_str = self._convert_wareki_to_date(
                getattr(issuance, 'maturity_date', None)
            )
            payment_date_str = self._convert_wareki_to_date(
                getattr(issuance, 'payment_date', None)
            )
            
            data = {
                'issuance_id': issuance_id,
                'announcement_id': announcement_id,
                'bond_master_id': self._get_bond_master_id(issuance),
                'issuance_date': issuance_date_str,
                'maturity_date': maturity_date_str,
                'interest_rate': getattr(issuance, 'interest_rate', None),
                'issue_price': getattr(issuance, 'issue_price', None),
                'issue_amount': getattr(issuance, 'issue_amount', None),
                'payment_date': payment_date_str,
                'created_at': datetime.utcnow(),
                'updated_at': datetime.utcnow()
            }
            result.append(data)
        
        return result
    
    def prepare_legal_basis_data(self, issuance_data: List[Dict], issuances: List) -> List[Dict]:
        """æ³•ä»¤æ ¹æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        result = []
        
        for issuance_dict, issuance_obj in zip(issuance_data, issuances):
            legal_basis = getattr(issuance_obj, 'legal_basis', None)
            if not legal_basis:
                continue
            
            law_id, article_id = self._parse_legal_basis(legal_basis)
            
            if law_id and article_id:
                result.append({
                    'issuance_id': issuance_dict['issuance_id'],
                    'law_id': law_id,
                    'article_id': article_id,
                    'created_at': datetime.utcnow()
                })
        
        return result
    
    def insert_to_bigquery(self, table_name: str, data: List[Dict]) -> int:
        """BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ï¼ˆå¼·åŒ–ã•ã‚ŒãŸãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
        if not data:
            return 0
        
        table_id = f"{self.client.project}.{self.dataset_id}.{table_name}"
        df = pd.DataFrame(data)
        
        if table_name == 'announcements':
            df['kanpo_date'] = pd.to_datetime(df['kanpo_date'], errors='coerce').dt.date
            df['announcement_number'] = df['announcement_number'].astype(str)
            df['gazette_issue_number'] = df['gazette_issue_number'].fillna('').astype(str)
            df['title'] = df['title'].fillna('').astype(str)
        
        elif table_name == 'bond_issuances':
            # æ—¢ã«è¥¿æš¦å½¢å¼ï¼ˆYYYY-MM-DDï¼‰ã®æ–‡å­—åˆ—ãªã®ã§ã€ãã®ã¾ã¾å¤‰æ›
            df['issuance_date'] = pd.to_datetime(df['issuance_date'], errors='coerce').dt.date
            df['maturity_date'] = pd.to_datetime(df['maturity_date'], errors='coerce').dt.date
            df['payment_date'] = pd.to_datetime(df['payment_date'], errors='coerce').dt.date
            df['interest_rate'] = pd.to_numeric(df['interest_rate'], errors='coerce')
            df['issue_price'] = pd.to_numeric(df['issue_price'], errors='coerce')
            df['issue_amount'] = pd.to_numeric(df['issue_amount'], errors='coerce')
        
        print(f"  ğŸ“Š {table_name}: {len(df)} ä»¶ã‚’æŠ•å…¥ä¸­...")
        
        # å¼·åŒ–ã•ã‚ŒãŸãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ5å›ã€æœ€å¤§48ç§’å¾…æ©Ÿï¼‰
        max_retries = 5
        base_wait_time = 3
        
        for attempt in range(max_retries):
            try:
                job_config = bigquery.LoadJobConfig(
                    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]
                )
                
                job = self.client.load_table_from_dataframe(df, table_id, job_config=job_config)
                job.result()
                
                print(f"  âœ… {table_name}: æŠ•å…¥æˆåŠŸ")
                return len(data)
                
            except Exception as e:
                error_msg = str(e)
                
                if '503' in error_msg or 'Service Unavailable' in error_msg or 'timeout' in error_msg.lower():
                    if attempt < max_retries - 1:
                        wait_time = base_wait_time * (2 ** attempt)
                        print(f"  âš ï¸ ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries})")
                        print(f"  â³ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"  âŒ {table_name}: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ")
                        raise
                else:
                    print(f"  âŒ {table_name}: æŠ•å…¥å¤±æ•— - {error_msg[:200]}")
                    raise
    
    def process_files(self, files: List[Path]):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†"""
        print("\n" + "="*60)
        print("ğŸ“¦ ç™ºè¡Œãƒ‡ãƒ¼ã‚¿æŠ•å…¥é–‹å§‹")
        print("="*60)
        
        announcements_buffer = []
        issuances_buffer = []
        legal_basis_buffer = []
        
        BATCH_SIZE = 10
        
        for idx, file_path in enumerate(files, 1):
            print(f"\n--- ãƒ•ã‚¡ã‚¤ãƒ« {idx}/{len(files)} ---")
            
            try:
                parsed_data = self.parse_kanpo_file(file_path)
                if not parsed_data:
                    self.stats['files_failed'] += 1
                    continue
                
                announcement = self.prepare_announcement_data(parsed_data)
                announcement_id = announcement['announcement_id']
                
                print(f"  å‘Šç¤ºID: {announcement_id}")
                
                issuances = self.prepare_issuance_data(announcement_id, parsed_data['issuances'])
                legal_basis = self.prepare_legal_basis_data(issuances, parsed_data['issuances'])
                
                announcements_buffer.append(announcement)
                issuances_buffer.extend(issuances)
                legal_basis_buffer.extend(legal_basis)
                
                self.stats['files_processed'] += 1
                
                if len(announcements_buffer) >= BATCH_SIZE:
                    print(f"\nğŸ”„ ãƒãƒƒãƒæŠ•å…¥å®Ÿè¡Œ ({len(announcements_buffer)} å‘Šç¤º)")
                    self._flush_buffers(announcements_buffer, issuances_buffer, legal_basis_buffer)
                    announcements_buffer = []
                    issuances_buffer = []
                    legal_basis_buffer = []
                
            except Exception as e:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path.name}")
                print(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
                self.stats['files_failed'] += 1
                self.stats['errors'].append({'file': file_path.name, 'error': str(e)})
        
        if announcements_buffer:
            print(f"\nğŸ”„ æœ€çµ‚ãƒãƒƒãƒæŠ•å…¥ ({len(announcements_buffer)} å‘Šç¤º)")
            self._flush_buffers(announcements_buffer, issuances_buffer, legal_basis_buffer)
        
        self._print_summary()
    
    def _flush_buffers(self, announcements: List, issuances: List, legal_basis: List):
        """ãƒãƒƒãƒ•ã‚¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥"""
        if announcements:
            count = self.insert_to_bigquery('announcements', announcements)
            self.stats['announcements_inserted'] += count
        
        if issuances:
            count = self.insert_to_bigquery('bond_issuances', issuances)
            self.stats['issuances_inserted'] += count
        
        if legal_basis:
            count = self.insert_to_bigquery('issuance_legal_basis', legal_basis)
            self.stats['legal_basis_inserted'] += count
    
    def _print_summary(self):
        """å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š æŠ•å…¥çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"âœ… å‡¦ç†æˆåŠŸ: {self.stats['files_processed']} ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"âŒ å‡¦ç†å¤±æ•—: {self.stats['files_failed']} ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"ğŸ“ å‘Šç¤ºæŠ•å…¥: {self.stats['announcements_inserted']} ä»¶")
        print(f"ğŸ’° éŠ˜æŸ„æŠ•å…¥: {self.stats['issuances_inserted']} ä»¶")
        print(f"ğŸ“‹ æ³•ä»¤æ ¹æ‹ æŠ•å…¥: {self.stats['legal_basis_inserted']} ä»¶")
        
        if self.stats['errors']:
            print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´° ({len(self.stats['errors'])}ä»¶):")
            for error in self.stats['errors'][:5]:
                print(f"  - {error['file']}: {error['error']}")
            if len(self.stats['errors']) > 5:
                print(f"  ... ä»– {len(self.stats['errors'])-5} ä»¶")
    
    def _generate_announcement_id(self, announcement: Dict) -> str:
        """å‘Šç¤ºIDã‚’ç”Ÿæˆ"""
        kanpo_date = announcement.get('kanpo_date', '')
        kanpo_date = str(kanpo_date).replace('-', '') if kanpo_date else '00000000'
        
        announcement_num = announcement.get('announcement_number', '000')
        if 'ç¬¬' in str(announcement_num) and 'å·' in str(announcement_num):
            announcement_num = str(announcement_num).replace('ç¬¬', '').replace('å·', '').strip()
        
        return f"ANN_{kanpo_date}_{announcement_num}"
    
    def _get_bond_master_id(self, issuance) -> Optional[str]:
        """éŠ˜æŸ„ãƒã‚¹ã‚¿IDã‚’å–å¾—"""
        bond_type = getattr(issuance, 'bond_type', '')
        type_mapping = {
            'åˆ©ä»˜å›½å‚µ': 'BOND_001',
            'ç‰©ä¾¡é€£å‹•å›½å‚µ': 'BOND_003',
            'GXå‚µåˆ¸': 'BOND_013',
            'å›½åº«çŸ­æœŸè¨¼åˆ¸': 'BOND_014'
        }
        return type_mapping.get(bond_type, 'BOND_001')
    
    def _parse_legal_basis(self, legal_basis: str) -> tuple:
        """æ³•ä»¤æ ¹æ‹ ã‚’è§£æï¼ˆè¤‡æ•°æ³•ä»¤ã®é€£çµã«å¯¾å¿œï¼‰"""
        if not legal_basis or legal_basis == "ä¸æ˜":
            return (None, None)
        
        # ã€ŒåŠã³ã€ã€Œä¸¦ã³ã«ã€ã§è¤‡æ•°ã®æ³•ä»¤ãŒé€£çµã•ã‚Œã¦ã„ã‚‹å ´åˆã¯åˆ†å‰²
        # ã€Œä¸¦ã³ã«ã€ã¯å¤§ã‚°ãƒ«ãƒ¼ãƒ—ã€ã€ŒåŠã³ã€ã¯å°ã‚°ãƒ«ãƒ¼ãƒ—ã®åŒºåˆ‡ã‚Šã ãŒã€
        # Day 3ã§ã¯ç°¡æ˜“ç‰ˆã¨ã—ã¦ä¸¡æ–¹ã‚’åŒç­‰ã«æ‰±ã†
        legal_parts = re.split(r'(?:åŠã³|ä¸¦ã³ã«)', legal_basis)
        
        # å„ãƒ‘ãƒ¼ãƒˆã‚’è©¦è¡Œï¼ˆç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹ã‚’å„ªå…ˆï¼‰
        for part in legal_parts:
            part = part.strip()
            
            # æ¡é …ç•ªå·ã‚’æŠ½å‡ºï¼ˆç¬¬XXæ¡ã€ç¬¬XXæ¡ã®XXã€ç¬¬XXæ¡ç¬¬Xé …ï¼‰
            article_match = re.search(r'ç¬¬(\d+)æ¡(?:ã®(\d+))?', part)
            if not article_match:
                continue
            
            article_num = article_match.group(1)
            subsection = article_match.group(2)
            
            # 1. è²¡æ”¿æ³• + ç¬¬4æ¡ â†’ LAW_ZAISEI + ART_ZAISEI_4_1
            if 'è²¡æ”¿æ³•' in part and article_num == '4':
                return ('LAW_ZAISEI', 'ART_ZAISEI_4_1')
            
            # 2-4. ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹ + ç¬¬46æ¡/ç¬¬47æ¡/ç¬¬62æ¡
            if 'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹' in part:
                if article_num == '46':
                    return ('LAW_TOKUBETSU', 'ART_TOKUBETSU_46_1')
                elif article_num == '47':
                    return ('LAW_TOKUBETSU', 'ART_TOKUBETSU_47_1')
                elif article_num == '62':
                    return ('LAW_TOKUBETSU', 'ART_TOKUBETSU_62_1')
            
            # 5. è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹ + ç¬¬2æ¡/ç¬¬3æ¡
            if 'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹' in part:
                if article_num == '2':
                    return ('LAW_TOKUREIKOUSAI_H24', 'ART_TOKUREIKOUSAI_H24_2_1')
                elif article_num == '3':
                    # ç¬¬3æ¡ã®å ´åˆã€æ–°ã—ã„article_idãŒå¿…è¦ã ãŒã€ãƒã‚¹ã‚¿ã«å­˜åœ¨ã—ãªã„ãŸã‚
                    # ç¬¬2æ¡ã¨ã—ã¦æ‰±ã†ï¼ˆã¾ãŸã¯ None ã‚’è¿”ã™ï¼‰
                    return ('LAW_TOKUREIKOUSAI_H24', 'ART_TOKUREIKOUSAI_H24_2_1')
            
            # 6. è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹ + ç¬¬7æ¡
            if 'è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹' in part and article_num == '7':
                return ('LAW_GX', 'ART_GX_7_1')
            
            # 7. æƒ…å ±å‡¦ç†ã®ä¿ƒé€²ã«é–¢ã™ã‚‹æ³•å¾‹ + ç¬¬69æ¡
            if 'æƒ…å ±å‡¦ç†ã®ä¿ƒé€²ã«é–¢ã™ã‚‹æ³•å¾‹' in part and article_num == '69':
                return ('LAW_JOHO', 'ART_JOHO_69_1')
            
            # 8. å­ã©ã‚‚ãƒ»å­è‚²ã¦æ”¯æ´æ³• + ç¬¬71æ¡ã®26
            if 'å­ã©ã‚‚' in part and 'å­è‚²ã¦æ”¯æ´æ³•' in part and article_num == '71' and subsection == '26':
                return ('LAW_KODOMO', 'ART_KODOMO_71_26_1')
        
        return (None, None)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='ç™ºè¡Œãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥')
    parser.add_argument('--limit', type=int, default=None, help='å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®ä¸Šé™')
    parser.add_argument('--data-dir', type=str, default=DATA_DIR, help='ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸš€ ç™ºè¡Œãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("="*60)
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.data_dir}")
    if args.limit:
        print(f"âš ï¸ åˆ¶é™ãƒ¢ãƒ¼ãƒ‰: æœ€åˆã® {args.limit} ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†")
    print()
    
    try:
        loader = IssuanceDataLoader(PROJECT_ID, DATASET_ID, SERVICE_ACCOUNT_KEY)
        files = loader.get_kanpo_files(args.data_dir, args.limit)
        
        if not files:
            print("âŒ å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        loader.process_files(files)
        print("\nâœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
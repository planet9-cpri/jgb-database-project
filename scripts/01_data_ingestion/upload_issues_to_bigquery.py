"""
BigQueryéŠ˜æŸ„æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
Day 4ã§æŠ½å‡ºã—ãŸéŠ˜æŸ„ã‚’BigQueryã«æŠ•å…¥
bonds_master + bond_issuances ã®æ§‹é€ ã«å¯¾å¿œ
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
from google.cloud import bigquery
from google.oauth2 import service_account
import re

# è¨­å®š
PROJECT_ID = "jgb2023"
DATASET_ID = "20251019"
BOND_MASTER_TABLE = "bonds_master"
ISSUANCES_TABLE = "bond_issuances"

# å‚µåˆ¸ç¨®é¡ãƒã‚¹ã‚¿ãƒ¼ï¼ˆbonds_masterç”¨ï¼‰
BOND_TYPES = {
    'ï¼’å¹´': {  # å…¨è§’è¡¨è¨˜ï¼ˆãƒ‘ãƒ¼ã‚µãƒ¼ã®å‡ºåŠ›ã«åˆã‚ã›ã‚‹ï¼‰
        'bond_id': 'BOND_KENSETSU_2Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ2å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 2.0,
        'maturity_type': '2å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»2å¹´ç‰©',
        'is_active': True
    },
    'ï¼•å¹´': {  # å…¨è§’è¡¨è¨˜ï¼ˆãƒ‘ãƒ¼ã‚µãƒ¼ã®å‡ºåŠ›ã«åˆã‚ã›ã‚‹ï¼‰
        'bond_id': 'BOND_KENSETSU_5Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ5å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 5.0,
        'maturity_type': '5å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»5å¹´ç‰©',
        'is_active': True
    },
    '10å¹´': {
        'bond_id': 'BOND_KENSETSU_10Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ10å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 10.0,
        'maturity_type': '10å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»10å¹´ç‰©',
        'is_active': True
    },
    '20å¹´': {
        'bond_id': 'BOND_KENSETSU_20Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ20å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 20.0,
        'maturity_type': '20å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»20å¹´ç‰©ãƒ»åˆ©å­æ”¯æ‰•æœŸã¯6æœˆ20æ—¥ã¨12æœˆ20æ—¥',
        'is_active': True
    },
    '30å¹´': {
        'bond_id': 'BOND_KENSETSU_30Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ30å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 30.0,
        'maturity_type': '30å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»30å¹´ç‰©ãƒ»åˆ©å­æ”¯æ‰•æœŸã¯9æœˆ20æ—¥ã¨3æœˆ20æ—¥',
        'is_active': True
    },
    '40å¹´': {
        'bond_id': 'BOND_KENSETSU_40Y',
        'bond_name': 'åˆ©ä»˜å›½åº«å‚µåˆ¸ï¼ˆ40å¹´ï¼‰',
        'bond_type': 'åˆ©ä»˜å›½å‚µ',
        'maturity_years': 40.0,
        'maturity_type': '40å¹´',
        'issue_method': 'æµå‹•æ€§ä¾›çµ¦',
        'interest_type': 'åˆ©ä»˜',
        'interest_payment': 'å¹´2å›',
        'min_denomination': 50000,
        'description': 'å›ºå®šé‡‘åˆ©ã®åˆ©ä»˜å›½å‚µãƒ»40å¹´ç‰©ãƒ»åˆ©å­æ”¯æ‰•æœŸã¯9æœˆ20æ—¥ã¨3æœˆ20æ—¥',
        'is_active': True
    }
}


def load_extraction_results(json_path: str) -> Dict:
    """æŠ½å‡ºçµæœJSONã‚’èª­ã¿è¾¼ã¿"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_announcement_id_from_filename(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰announcement_idã‚’ç”Ÿæˆ
    ä¾‹: 20230414_ä»¤å’Œ5å¹´5æœˆ9æ—¥ä»˜ï¼ˆè²¡å‹™çœç¬¬ç™¾äºŒåä¸ƒå·ï¼‰.txt â†’ ANN_20230414_127
    """
    # æ—¥ä»˜éƒ¨åˆ†ï¼ˆYYYYMMDDï¼‰
    date_match = re.match(r'(\d{8})_', filename)
    if not date_match:
        return f"ANN_unknown_{filename[:20]}"
    
    date_part = date_match.group(1)
    
    # å‘Šç¤ºç•ªå·ã®æŠ½å‡ºï¼ˆæ¼¢æ•°å­—å¤‰æ›ãƒãƒƒãƒ—ï¼‰
    kanji_to_num = {
        'ç™¾äºŒåä¸ƒ': '127', 'ç™¾äºŒåå…­': '126', 'ç™¾äº”åå…«': '158', 'ç™¾äº”åä¹': '159',
        'ç™¾å…«åå…­': '186', 'ç™¾å…«åä¸ƒ': '187', 'äºŒç™¾ä¸€': '201', 'äºŒç™¾äºŒ': '202',
        'äºŒç™¾äºŒåäºŒ': '222', 'äºŒç™¾äºŒåä¸‰': '223', 'äºŒç™¾äº”åå››': '254', 'äºŒç™¾äº”åäº”': '255',
        'äºŒç™¾ä¸ƒåå››': '274', 'äºŒç™¾ä¸ƒåäº”': '275', 'ä¸‰ç™¾å…­': '306', 'ä¸‰ç™¾ä¸ƒ': '307',
        'ä¸ƒ': '7', 'å…«': '8', 'ä¸‰åå…«': '38', 'ä¸‰åä¹': '39', 'ä¸ƒåå››': '74',
        'ä¸ƒåä¸‰': '73', 'ç™¾åäºŒ': '112', 'ç™¾åä¸€': '111'
    }
    
    notice_match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)å·', filename)
    if notice_match:
        notice_str = notice_match.group(1)
        notice_num = kanji_to_num.get(notice_str, notice_str)
    else:
        notice_num = "unknown"
    
    return f"ANN_{date_part}_{notice_num}"


def get_issuance_date_from_filename(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç™ºè¡Œæ—¥ã‚’å–å¾—ï¼ˆYYYYMMDD â†’ YYYY-MM-DDï¼‰"""
    date_match = re.match(r'(\d{4})(\d{2})(\d{2})_', filename)
    if date_match:
        return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
    return None


def check_missing_bond_types(client: bigquery.Client, results: Dict) -> Set[str]:
    """
    bonds_masterã«ä¸è¶³ã—ã¦ã„ã‚‹å‚µåˆ¸ç¨®é¡ã‚’ç¢ºèª
    
    Returns:
        ä¸è¶³ã—ã¦ã„ã‚‹ç¨®é¡ã®ã‚»ãƒƒãƒˆï¼ˆä¾‹: {'10å¹´', '20å¹´'}ï¼‰
    """
    # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ç¨®é¡ã‚’åé›†
    used_types = set()
    for item in results['success']:
        for issue in item['issues']:
            used_types.add(issue['bond_type'])
    
    # æ—¢å­˜ã®bonds_masterã‚’ç¢ºèª
    query = f"""
    SELECT DISTINCT maturity_type
    FROM `{PROJECT_ID}.{DATASET_ID}.{BOND_MASTER_TABLE}`
    WHERE is_active = TRUE
    """
    
    existing_types = set()
    try:
        results_query = client.query(query).result()
        for row in results_query:
            existing_types.add(row.maturity_type)
    except Exception as e:
        print(f"âš ï¸ bonds_masterç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä¸è¶³ã—ã¦ã„ã‚‹ç¨®é¡
    missing = used_types - existing_types
    
    return missing


def insert_missing_bond_masters(client: bigquery.Client, missing_types: Set[str]):
    """ä¸è¶³ã—ã¦ã„ã‚‹å‚µåˆ¸ç¨®é¡ã‚’bonds_masterã«è¿½åŠ """
    if not missing_types:
        return
    
    print(f"\nğŸ“ bonds_masterã«{len(missing_types)}ç¨®é¡ã‚’è¿½åŠ ã—ã¾ã™")
    
    rows = []
    for bond_type in missing_types:
        if bond_type in BOND_TYPES:
            master_data = BOND_TYPES[bond_type].copy()
            master_data['created_at'] = datetime.now().isoformat()
            master_data['updated_at'] = datetime.now().isoformat()
            rows.append(master_data)
            print(f"  - {master_data['bond_name']}")
    
    if rows:
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.{BOND_MASTER_TABLE}"
        errors = client.insert_rows_json(table_ref, rows)
        
        if errors:
            print(f"âŒ bonds_masteræŠ•å…¥ã‚¨ãƒ©ãƒ¼:")
            for error in errors:
                print(f"  {error}")
            raise Exception("bonds_masteræŠ•å…¥å¤±æ•—")
        else:
            print(f"âœ… bonds_masterã«{len(rows)}ä»¶è¿½åŠ ã—ã¾ã—ãŸ")


def prepare_issuance_rows(results: Dict) -> List[Dict]:
    """
    bond_issuancesæŠ•å…¥ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    
    Returns:
        BigQueryè¡Œãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    rows = []
    
    for item in results['success']:
        filename = item['filename']
        announcement_id = get_announcement_id_from_filename(filename)
        issuance_date_str = get_issuance_date_from_filename(filename)
        
        for issue in item['issues']:
            # maturity_dateå¤‰æ›
            maturity_date = None
            if issue.get('maturity_date'):
                try:
                    dt = datetime.fromisoformat(issue['maturity_date'])
                    maturity_date = dt.strftime('%Y-%m-%d')
                except:
                    maturity_date = None
            
            # bond_master_idã‚’å–å¾—
            bond_type = issue['bond_type']
            if bond_type in BOND_TYPES:
                bond_master_id = BOND_TYPES[bond_type]['bond_id']
            else:
                bond_master_id = f"BOND_UNKNOWN_{bond_type}"
            
            # issuance_idç”Ÿæˆ
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: ANN_YYYYMMDD_XXX_ISSUE_YYY
            # ä¾‹: ANN_20230414_127_ISSUE_167
            series_num = issue['series_number']
            issuance_id = f"{announcement_id}_ISSUE_{series_num:03d}"
            
            row = {
                'issuance_id': issuance_id,
                'announcement_id': announcement_id,
                'bond_master_id': bond_master_id,
                'issuance_date': issuance_date_str,
                'maturity_date': maturity_date,
                'interest_rate': float(issue['rate']) if issue.get('rate') else None,
                'issue_price': None,  # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã«ã¯ãªã„ã®ã§NULL
                'issue_amount': int(issue['amount']) if issue.get('amount') else None,
                'payment_date': issuance_date_str,  # ç™ºè¡Œæ—¥ã¨åŒã˜ã¨ä»®å®š
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }
            
            rows.append(row)
    
    return rows


def upload_to_bigquery(rows: List[Dict], credentials_path: str = None):
    """BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    print("\n" + "=" * 70)
    print("ğŸš€ BigQueryæŠ•å…¥é–‹å§‹")
    print("=" * 70)
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_ID}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {ISSUANCES_TABLE}")
    print(f"æŠ•å…¥è¡Œæ•°: {len(rows)}")
    print()
    
    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    if credentials_path:
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/bigquery"]
        )
        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)
    else:
        client = bigquery.Client(project=PROJECT_ID)
    
    table_ref = f"{PROJECT_ID}.{DATASET_ID}.{ISSUANCES_TABLE}"
    
    try:
        # ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
        print("æŠ•å…¥ä¸­...")
        errors = client.insert_rows_json(table_ref, rows)
        
        if errors:
            print("\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:")
            for error in errors:
                print(f"  {error}")
            return False
        else:
            print(f"\nâœ… æŠ•å…¥æˆåŠŸï¼š{len(rows)}è¡Œ")
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print("\nã€æŠ•å…¥ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰ã€‘")
            for i, row in enumerate(rows[:3], 1):
                print(f"\n{i}. issuance_id: {row['issuance_id']}")
                print(f"   announcement_id: {row['announcement_id']}")
                print(f"   bond_master_id: {row['bond_master_id']}")
                print(f"   åˆ©ç‡: {row['interest_rate']}%, ç™ºè¡Œé¡: {row['issue_amount']:,}å††")
            
            if len(rows) > 3:
                print(f"\n   ... ä»–{len(rows) - 3}ä»¶")
            
            return True
    
    except Exception as e:
        print(f"\nâŒ BigQueryã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_upload(credentials_path: str = None):
    """æŠ•å…¥å¾Œã®ç¢ºèªã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æŠ•å…¥çµæœã®ç¢ºèª")
    print("=" * 70)
    
    if credentials_path:
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/bigquery"]
        )
        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)
    else:
        client = bigquery.Client(project=PROJECT_ID)
    
    # ç™ºè¡Œä»¶æ•°ã®ç¢ºèª
    query = f"""
    SELECT 
        COUNT(*) as total_issuances,
        COUNT(DISTINCT announcement_id) as total_announcements,
        COUNT(DISTINCT bond_master_id) as total_bond_types
    FROM `{PROJECT_ID}.{DATASET_ID}.{ISSUANCES_TABLE}`
    """
    
    try:
        results = client.query(query).result()
        for row in results:
            print(f"\nç·ç™ºè¡Œä»¶æ•°: {row.total_issuances}")
            print(f"ç·å‘Šç¤ºæ•°: {row.total_announcements}")
            print(f"å‚µåˆ¸ç¨®é¡æ•°: {row.total_bond_types}")
        
        # ç¨®é¡åˆ¥é›†è¨ˆ
        query2 = f"""
        SELECT 
            bm.bond_name,
            COUNT(*) as count
        FROM `{PROJECT_ID}.{DATASET_ID}.{ISSUANCES_TABLE}` bi
        LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.{BOND_MASTER_TABLE}` bm
            ON bi.bond_master_id = bm.bond_id
        GROUP BY bm.bond_name
        ORDER BY bm.bond_name
        """
        
        print("\nã€ç¨®é¡åˆ¥é›†è¨ˆã€‘")
        results2 = client.query(query2).result()
        for row in results2:
            bond_name = row.bond_name or "ä¸æ˜"
            print(f"  {bond_name}: {row.count}ä»¶")
        
        return True
    
    except Exception as e:
        print(f"\nâŒ ç¢ºèªã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # æŠ½å‡ºçµæœã®èª­ã¿è¾¼ã¿
    json_path = "extraction_results.json"
    
    if not Path(json_path).exists():
        print(f"âŒ {json_path}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 1
    
    print("ğŸ“„ æŠ½å‡ºçµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")
    results = load_extraction_results(json_path)
    
    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"  æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«: {len(results['success'])}ä»¶")
    print(f"  ç·éŠ˜æŸ„æ•°: {results['summary']['total_issues']}")
    print()
    
    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    credentials_path = None  # ã¾ãŸã¯ "path/to/service-account-key.json"
    
    if credentials_path:
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/bigquery"]
        )
        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)
    else:
        client = bigquery.Client(project=PROJECT_ID)
    
    # bonds_masterã®ä¸è¶³ç¢ºèª
    print("ğŸ“‹ bonds_masterã®ç¢ºèªä¸­...")
    missing_types = check_missing_bond_types(client, results)
    
    if missing_types:
        print(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹å‚µåˆ¸ç¨®é¡: {missing_types}")
        insert_missing_bond_masters(client, missing_types)
    else:
        print("âœ… bonds_masterã¯å®Œå…¨ã§ã™")
    
    # bond_issuancesæŠ•å…¥ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    print("\nğŸ“ bond_issuancesæŠ•å…¥ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
    rows = prepare_issuance_rows(results)
    print(f"âœ… æº–å‚™å®Œäº†ï¼š{len(rows)}è¡Œ")
    print()
    
    # ç¢ºèª
    response = input(f"BigQueryã«{len(rows)}è¡Œã‚’æŠ•å…¥ã—ã¾ã™ã‹ï¼Ÿ (yes/no): ")
    if response.lower() != 'yes':
        print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return 0
    
    # BigQueryã«æŠ•å…¥
    success = upload_to_bigquery(rows, credentials_path)
    
    if success:
        # æŠ•å…¥çµæœã®ç¢ºèª
        verify_upload(credentials_path)
        
        print("\n" + "=" * 70)
        print("ğŸ‰ Day 4å®Œäº†ï¼")
        print("=" * 70)
        print(f"âœ… 24ä»¶ã®å‘Šç¤ºã‹ã‚‰717éŠ˜æŸ„ã‚’æŠ½å‡º")
        print(f"âœ… {len(rows)}ä»¶ã‚’BigQueryã«æŠ•å…¥")
        print(f"âœ… é”æˆç‡: 100%")
        print()
        
        return 0
    else:
        print("\nâš ï¸ æŠ•å…¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
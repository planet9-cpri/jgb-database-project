"""
ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«æŠ•å…¥ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python scripts/load_masters.py

æ©Ÿèƒ½:
    - data/masters/ é…ä¸‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’BigQueryã«æŠ•å…¥
    - ãƒ‡ãƒ¼ã‚¿å‹ã‚’è‡ªå‹•å¤‰æ›ï¼ˆBOOLEAN, DATE, INTEGER, NUMERICï¼‰
    - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆcreated_at, updated_atï¼‰ã‚’è‡ªå‹•è¿½åŠ 
    - æŠ•å…¥å¾Œã«ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œ
"""

from google.cloud import bigquery
from pathlib import Path
import pandas as pd
from datetime import datetime
import numpy as np

# ==================== è¨­å®š ====================

credentials_path = r"C:\Users\sonke\secrets\jgb2023-f8c9b849ae2d.json"
project_id = "jgb2023"
dataset_id = "20251019"
masters_dir = Path("data/masters")

# ==================== é–¢æ•°å®šç¾© ====================

def convert_data_types(df, table_name):
    """
    DataFrameã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’BigQueryã®ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦å¤‰æ›
    
    Args:
        df: pandas DataFrame
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        
    Returns:
        å¤‰æ›å¾Œã®DataFrame
    """
    print(f"  ğŸ”„ ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›ä¸­...")
    
    # BOOLEANå‹ã®å¤‰æ›
    if 'is_active' in df.columns:
        df['is_active'] = df['is_active'].map({
            'TRUE': True, 'True': True, 'true': True,
            'FALSE': False, 'False': False, 'false': False,
            True: True, False: False
        })
        print(f"    âœ“ is_active â†’ BOOLEAN")
    
    # DATEå‹ã®å¤‰æ›
    date_columns = ['promulgation_date', 'enforcement_date', 'effective_from', 'effective_to']
    for col in date_columns:
        if col in df.columns:
            df[col] = df[col].replace('', pd.NA)
            df[col] = pd.to_datetime(df[col], errors='coerce')
            print(f"    âœ“ {col} â†’ DATE")
    
    # INTEGERå‹ã®å¤‰æ›ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
    if 'maturity_years' in df.columns:
        try:
            # ç©ºæ–‡å­—åˆ—ã¨Noneã‚’NaNã«å¤‰æ›
            df['maturity_years'] = df['maturity_years'].replace(['', None, 'nan', 'NaN'], pd.NA)
            
            # ã¾ãšæ•°å€¤å‹ã«å¤‰æ›
            df['maturity_years'] = pd.to_numeric(df['maturity_years'], errors='coerce')
            
            # NaNã§ãªã„å€¤ã‚’æ•´æ•°ã«ä¸¸ã‚ã‚‹ï¼ˆå°æ•°ç‚¹ä»¥ä¸‹ã‚’å‰Šé™¤ï¼‰
            # ã“ã‚Œã§float64 -> int64ã®å¤‰æ›ãŒå®‰å…¨ã«ãªã‚‹
            df['maturity_years'] = df['maturity_years'].apply(
                lambda x: pd.NA if pd.isna(x) else int(round(x))
            )
            
            # Int64å‹ã«å¤‰æ›
            df['maturity_years'] = df['maturity_years'].astype('Int64')
            
            print(f"    âœ“ maturity_years â†’ INTEGER")
        except Exception as e:
            print(f"    âš ï¸  maturity_yearså¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãã®ã¾ã¾ç¶™ç¶š
    
    # NUMERICå‹ã®å¤‰æ›
    if 'min_denomination' in df.columns:
        df['min_denomination'] = df['min_denomination'].replace(['', None], pd.NA)
        df['min_denomination'] = pd.to_numeric(df['min_denomination'], errors='coerce')
        print(f"    âœ“ min_denomination â†’ NUMERIC")
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
    current_time = pd.Timestamp.now(tz='UTC')
    if 'created_at' not in df.columns:
        df['created_at'] = current_time
        print(f"    âœ“ created_at â†’ TIMESTAMP (è‡ªå‹•è¿½åŠ )")
    if 'updated_at' not in df.columns:
        df['updated_at'] = current_time
        print(f"    âœ“ updated_at â†’ TIMESTAMP (è‡ªå‹•è¿½åŠ )")
    
    # ç©ºæ–‡å­—åˆ—ã‚’Noneã«å¤‰æ›
    df = df.replace('', None)
    
    return df


def load_master_table(client, table_name, csv_file):
    """
    å˜ä¸€ã®ãƒã‚¹ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’BigQueryã«æŠ•å…¥
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        csv_file: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        (æˆåŠŸ/å¤±æ•—, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    print(f"\nğŸ“¦ {table_name} ã‚’æŠ•å…¥ä¸­...")
    print(f"  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    
    try:
        # CSVã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_file)
        print(f"  âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}è¡Œ")
        print(f"  ğŸ“Š ã‚«ãƒ©ãƒ : {list(df.columns)}")
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
        df = convert_data_types(df, table_name)
        
        # BigQueryãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§
        table_ref = f"{project_id}.{dataset_id}.{table_name}"
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
        print(f"  ğŸš€ BigQueryã«æŠ•å…¥ä¸­...")
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ã
            autodetect=False,  # è‡ªå‹•æ¤œå‡ºã‚’ç„¡åŠ¹åŒ–ï¼ˆã‚¹ã‚­ãƒ¼ãƒã¯æ—¢ã«å®šç¾©æ¸ˆã¿ï¼‰
        )
        
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()  # å®Œäº†ã‚’å¾…ã¤
        
        print(f"  âœ… æŠ•å…¥æˆåŠŸ: {len(df)}è¡Œ")
        return True, None
        
    except Exception as e:
        print(f"  âŒ æŠ•å…¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False, str(e)


def verify_data(client, table_name):
    """
    æŠ•å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼
    
    Args:
        client: BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    table_ref = f"{project_id}.{dataset_id}.{table_name}"
    
    try:
        # è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        query = f"""
        SELECT COUNT(*) as row_count
        FROM `{table_ref}`
        """
        
        result = client.query(query).to_dataframe()
        row_count = result['row_count'].iloc[0]
        
        print(f"ğŸ“Š {table_name}:")
        print(f"  è¡Œæ•°: {row_count}")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        if row_count > 0:
            sample_query = f"""
            SELECT *
            FROM `{table_ref}`
            LIMIT 3
            """
            sample_df = client.query(sample_query).to_dataframe()
            print(f"  ã‚«ãƒ©ãƒ æ•°: {len(sample_df.columns)}")
            print(f"  ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿(æœ€åˆã®3è¡Œ):")
            print(sample_df.to_string(index=False))
        else:
            # ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã ã‘è¡¨ç¤º
            table = client.get_table(table_ref)
            print(f"  ã‚«ãƒ©ãƒ æ•°: {len(table.schema)}")
            print(f"  ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: ãªã—")
    
    except Exception as e:
        print(f"  âš ï¸  æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")


# ==================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ====================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸš€ ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿BigQueryæŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("="*60)
    print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_id}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_id}")
    print(f"èªè¨¼æƒ…å ±: {credentials_path}")
    print(f"ãƒã‚¹ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {masters_dir}")
    
    # BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
    print("\nğŸ”Œ BigQueryã«æ¥ç¶šä¸­...")
    client = bigquery.Client.from_service_account_json(credentials_path, project=project_id)
    print(f"âœ… æ¥ç¶šæˆåŠŸ: {client.project}")
    
    # ãƒã‚¹ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®å®šç¾©ï¼ˆæŠ•å…¥é †åºã‚‚è€ƒæ…®ï¼‰
    master_tables = [
        "laws_master",
        "law_articles_master", 
        "bonds_master"
    ]
    
    # æŠ•å…¥å‡¦ç†
    print("\n" + "="*60)
    print("ğŸ“¦ ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ•å…¥é–‹å§‹")
    print("="*60)
    
    success_count = 0
    failed_tables = []
    
    for table_name in master_tables:
        csv_file = masters_dir / f"{table_name}.csv"
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not csv_file.exists():
            print(f"\nâš ï¸  {table_name}: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"    {csv_file}")
            failed_tables.append((table_name, "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"))
            continue
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ•å…¥
        success, error = load_master_table(client, table_name, csv_file)
        
        if success:
            success_count += 1
        else:
            failed_tables.append((table_name, error))
    
    # æŠ•å…¥å®Œäº†
    print("\n" + "="*60)
    print("âœ… ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Œäº†")
    print(f"  æˆåŠŸ: {success_count} / {len(master_tables)}")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    print("\n" + "="*60)
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
    print("="*60)
    
    for table_name in master_tables:
        verify_data(client, table_name)
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print("ğŸ“Š æŠ•å…¥çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f"âœ… æˆåŠŸ: {success_count} ãƒ†ãƒ¼ãƒ–ãƒ«")
    
    if failed_tables:
        print(f"âŒ å¤±æ•—: {len(failed_tables)} ãƒ†ãƒ¼ãƒ–ãƒ«")
        for table_name, error in failed_tables:
            print(f"  - {table_name}: {error}")
        print("\nâš ï¸  ä¸€éƒ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®æŠ•å…¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("   ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
    else:
        print("ğŸ‰ å…¨ã¦ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«æŠ•å…¥ã•ã‚Œã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
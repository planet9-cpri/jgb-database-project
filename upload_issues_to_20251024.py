"""
ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ20251024ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰

æ”¹å–„ç‰ˆï¼šç™ºè¡Œæ ¹æ‹ æ³•ä»¤ã®æŠ½å‡ºã¨åˆ†é›¢ã«å¯¾å¿œï¼ˆ2023å¹´åº¦ç‰ˆï¼‰
éšå±¤æ§‹é€ ï¼šcategoryï¼ˆå¤§åˆ†é¡ï¼‰ã€sub_categoryï¼ˆè©³ç´°åˆ†é¡ï¼‰

æ”¿åºœçŸ­æœŸè¨¼åˆ¸ãŒå«ã¾ã‚Œã‚‹å‘Šç¤ºã§ã‚‚ã€å‰²å¼•çŸ­æœŸå›½å‚µã¯æŠ½å‡ºã™ã‚‹
"""

import os
import sys
import re
import json
from datetime import datetime, date, timedelta
import uuid

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from google.cloud import bigquery

# ãƒ‘ãƒ¼ã‚µãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
try:
    from parsers.table_parser import TableParser
    from parsers.issue_extractor import IssueExtractor
    HAS_PARSERS = True
except ImportError:
    print("âš ï¸  è­¦å‘Š: parsersãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
    HAS_PARSERS = False

# BigQueryè¨­å®š
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r"C:\Users\sonke\secrets\jgb2023-f8c9b849ae2d.json"
PROJECT_ID = 'jgb2023'
DATASET_ID = '20251024'

client = bigquery.Client(project=PROJECT_ID)

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\JGBãƒ‡ãƒ¼ã‚¿\2023"

# ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ2023å¹´åº¦ç”¨ãƒ»éšå±¤æ§‹é€ ç‰ˆï¼‰
LEGAL_BASIS_PATTERNS = {
    '4æ¡å‚µ': {
        'category': '4æ¡å‚µ',
        'sub_category': 'å»ºè¨­å›½å‚µ',
        'patterns': [
            r'è²¡æ”¿æ³•.*?ç¬¬4æ¡ç¬¬1é …',
            r'è²¡æ”¿æ³•.*?ç¬¬å››æ¡ç¬¬ä¸€é …'
        ]
    },
    'ç‰¹ä¾‹å‚µ': {
        'category': 'ç‰¹ä¾‹å‚µ',
        'sub_category': 'ç‰¹ä¾‹å…¬å‚µ',
        'patterns': [
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬2æ¡ç¬¬1é …',
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬äºŒæ¡ç¬¬ä¸€é …',
            r'å¹³æˆ24å¹´æ³•å¾‹ç¬¬101å·.*?ç¬¬2æ¡ç¬¬1é …'
        ]
    },
    'GXçµŒæ¸ˆç§»è¡Œå‚µ': {
        'category': 'GXçµŒæ¸ˆç§»è¡Œå‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬7æ¡ç¬¬1é …',
            r'è„±ç‚­ç´ æˆé•·å‹çµŒæ¸ˆæ§‹é€ ã¸ã®å††æ»‘ãªç§»è¡Œã®æ¨é€²ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬ä¸ƒæ¡ç¬¬ä¸€é …',
            r'ä»¤å’Œ5å¹´æ³•å¾‹ç¬¬32å·.*?ç¬¬7æ¡ç¬¬1é …'
        ]
    },
    'å¾©èˆˆå‚µ': {
        'category': 'å¾©èˆˆå‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'æ±æ—¥æœ¬å¤§éœ‡ç½ã‹ã‚‰ã®å¾©èˆˆã®ãŸã‚ã®æ–½ç­–ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã«é–¢ã™ã‚‹ç‰¹åˆ¥æªç½®æ³•.*?ç¬¬69æ¡ç¬¬4é …',
            r'æ±æ—¥æœ¬å¤§éœ‡ç½ã‹ã‚‰ã®å¾©èˆˆã®ãŸã‚ã®æ–½ç­–ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã«é–¢ã™ã‚‹ç‰¹åˆ¥æªç½®æ³•.*?ç¬¬å…­åä¹æ¡ç¬¬å››é …'
        ]
    },
    'å¹´é‡‘ç‰¹ä¾‹å‚µ': {
        'category': 'å¹´é‡‘ç‰¹ä¾‹å‚µ',
        'sub_category': 'ã¤ãªãå…¬å‚µ',
        'patterns': [
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬3æ¡ç¬¬1é …',
            r'è²¡æ”¿é‹å–¶ã«å¿…è¦ãªè²¡æºã®ç¢ºä¿ã‚’å›³ã‚‹ãŸã‚ã®å…¬å‚µã®ç™ºè¡Œã®ç‰¹ä¾‹ã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬ä¸‰æ¡ç¬¬ä¸€é …',
            r'å¹³æˆ24å¹´æ³•å¾‹ç¬¬101å·.*?ç¬¬3æ¡ç¬¬1é …'
        ]
    },
    'å€Ÿæ›å‚µ': {
        'category': 'å€Ÿæ›å‚µ',
        'sub_category': 'å€Ÿæ›å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬46æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å››åå…­æ¡ç¬¬ä¸€é …'
        ]
    },
    'å‰å€’å‚µ': {
        'category': 'å€Ÿæ›å‚µ',
        'sub_category': 'å‰å€’å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬47æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å››åä¸ƒæ¡ç¬¬ä¸€é …'
        ]
    },
    'è²¡æŠ•å‚µ': {
        'category': 'è²¡æŠ•å‚µ',
        'sub_category': 'è²¡æŠ•å‚µ',
        'patterns': [
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬62æ¡ç¬¬1é …',
            r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬å…­åäºŒæ¡ç¬¬ä¸€é …'
        ]
    }
}

# æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé™¤å¤–ç”¨ï¼‰
SEIFU_TANKI_PATTERNS = [
    r'è²¡æ”¿æ³•.*?ç¬¬7æ¡ç¬¬1é …',
    r'è²¡æ”¿èè³‡è³‡é‡‘æ³•.*?ç¬¬9æ¡ç¬¬1é …',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬83æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬94æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬95æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬136æ¡',
    r'ç‰¹åˆ¥ä¼šè¨ˆã«é–¢ã™ã‚‹æ³•å¾‹.*?ç¬¬137æ¡'
]


def parse_date_string(date_value):
    """æ—¥ä»˜æ–‡å­—åˆ—ã¾ãŸã¯dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    if date_value is None:
        return None
    
    if isinstance(date_value, date):
        return date_value
    
    if isinstance(date_value, str):
        try:
            return datetime.strptime(date_value, '%Y-%m-%d').date()
        except:
            try:
                return datetime.strptime(date_value, '%Y/%m/%d').date()
            except:
                return None
    
    return None


def extract_legal_bases(text):
    """ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å¯¾å¿œã€é‡‘é¡ã‚‚æŠ½å‡ºï¼‰"""
    results = []
    
    for basis_name, config in LEGAL_BASIS_PATTERNS.items():
        for pattern in config['patterns']:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                full_text = match.group(0)
                amount = extract_amount_for_legal_basis(text, full_text)
                
                results.append({
                    'basis': basis_name,
                    'category': config['category'],
                    'sub_category': config['sub_category'],
                    'full': full_text,
                    'amount': amount
                })
                break
    
    return results


def extract_amount_for_legal_basis(text, legal_text):
    """ç‰¹å®šã®ç™ºè¡Œæ ¹æ‹ ã«ç´ã¥ãé‡‘é¡ã‚’æŠ½å‡º"""
    pattern1 = re.escape(legal_text) + r'[^å††]*?é¡é¢é‡‘é¡ã§([\d,]+)å††'
    match = re.search(pattern1, text, re.IGNORECASE | re.DOTALL)
    
    if match:
        return int(match.group(1).replace(',', ''))
    
    pattern2 = re.escape(legal_text) + r'[^å††]*?é‡‘é¡([\d,]+)å††'
    match = re.search(pattern2, text, re.IGNORECASE | re.DOTALL)
    
    if match:
        return int(match.group(1).replace(',', ''))
    
    return None


def has_waribiki_tanki(text):
    """
    å‰²å¼•çŸ­æœŸå›½å‚µãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š
    
    ã€Œå‰²å¼•çŸ­æœŸå›½å‚µã€ã¨ã„ã†æ˜ç¤ºçš„ãªè¨˜è¿°ã‚’æ¢ã™
    """
    return 'å‰²å¼•çŸ­æœŸå›½å‚µ' in text


def has_government_short_term_bond(text):
    """æ”¿åºœçŸ­æœŸè¨¼åˆ¸ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    has_seifu_pattern = any(re.search(p, text, re.IGNORECASE) for p in SEIFU_TANKI_PATTERNS)
    has_seifu_word = 'æ”¿åºœçŸ­æœŸè¨¼åˆ¸' in text
    return has_seifu_pattern or has_seifu_word


def extract_series_number(text):
    """å›å·ã‚’æŠ½å‡º"""
    match = re.search(r'ç¬¬(\d+)å›', text)
    if match:
        return f"ç¬¬{match.group(1)}å›"
    return None


def calculate_maturity_period(issuance_date, maturity_date):
    """å„Ÿé‚„æœŸé–“ã‚’è¨ˆç®—"""
    issuance_date = parse_date_string(issuance_date)
    maturity_date = parse_date_string(maturity_date)
    
    if not issuance_date or not maturity_date:
        return None, None, None
    
    delta = maturity_date - issuance_date
    days = delta.days
    months = days / 30.0
    years = days / 365.0
    
    return days, months, years


def determine_bond_master_id(bond_name, maturity_days):
    """å‚µåˆ¸åã¨å„Ÿé‚„æœŸé–“ã‹ã‚‰é©åˆ‡ãªbond_master_idã‚’æ±ºå®š"""
    if 'çŸ­æœŸ' in bond_name or 'TANKI' in bond_name:
        if maturity_days and maturity_days >= 270:
            return 'BOND_TANKI_1Y'
        else:
            return 'BOND_TANKI_6M'
    
    if 'ã‚¯ãƒ©ã‚¤ãƒ¡ãƒ¼ãƒˆ' in bond_name or 'ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³' in bond_name:
        if '10å¹´' in bond_name:
            return 'BOND_GX_10Y'
        elif '5å¹´' in bond_name:
            return 'BOND_GX_5Y'
    
    mapping = {
        '2å¹´': 'BOND_002Y',
        '5å¹´': 'BOND_005Y',
        '10å¹´': 'BOND_010Y',
        '20å¹´': 'BOND_020Y',
        '30å¹´': 'BOND_030Y',
        '40å¹´': 'BOND_040Y',
        'å›ºå®šãƒ»3å¹´': 'BOND_KOJIN_FIXED_3Y',
        'å›ºå®šãƒ»5å¹´': 'BOND_KOJIN_FIXED_5Y',
        'å¤‰å‹•ãƒ»10å¹´': 'BOND_KOJIN_VARIABLE_10Y',
        'ç‰©ä¾¡é€£å‹•': 'BOND_BUKKA_10Y'
    }
    
    for key, value in mapping.items():
        if key in bond_name:
            return value
    
    return None


def get_issuance_attribute(issuance, attr_name):
    """BondIssuanceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å±æ€§ã‚’å®‰å…¨ã«å–å¾—"""
    possible_names = {
        'issuance_date': ['issuance_date', 'issue_date', 'issueDate'],
        'maturity_date': ['maturity_date', 'maturityDate'],
        'payment_date': ['payment_date', 'paymentDate'],
        'issue_amount': ['issue_amount', 'issueAmount', 'face_value_individual'],
        'issue_price': ['issue_price', 'issuePrice'],
        'interest_rate': ['interest_rate', 'interestRate'],
        'series_number': ['series_number', 'seriesNumber'],
        'bond_name': ['bond_name', 'bondName']
    }
    
    if attr_name in possible_names:
        for name in possible_names[attr_name]:
            if hasattr(issuance, name):
                return getattr(issuance, name)
    
    if hasattr(issuance, attr_name):
        return getattr(issuance, attr_name)
    
    return None


def split_by_legal_basis(issuance, legal_bases, announcement_text):
    """1ã¤ã®ç™ºè¡Œã‚’è¤‡æ•°ã®ç™ºè¡Œæ ¹æ‹ ã«åˆ†é›¢"""
    if len(legal_bases) == 0:
        return [issuance]
    
    if len(legal_bases) == 1:
        issuance['legal_basis'] = legal_bases[0]['basis']
        issuance['legal_basis_full'] = legal_bases[0]['full']
        issuance['legal_basis_category'] = legal_bases[0]['category']
        return [issuance]
    
    split_issuances = []
    total_extracted_amount = sum([lb['amount'] for lb in legal_bases if lb['amount']])
    original_amount = issuance['issue_amount']
    
    if total_extracted_amount > 0 and original_amount and abs(total_extracted_amount - original_amount) > original_amount * 0.01:
        print(f"  âš ï¸  é‡‘é¡ä¸ä¸€è‡´: åˆè¨ˆ{total_extracted_amount:,}å†† vs å…ƒ{original_amount:,}å††")
    
    for lb in legal_bases:
        new_issuance = issuance.copy()
        new_issuance['issuance_id'] = str(uuid.uuid4())
        new_issuance['legal_basis'] = lb['basis']
        new_issuance['legal_basis_full'] = lb['full']
        new_issuance['legal_basis_category'] = lb['category']
        new_issuance['is_split'] = True
        new_issuance['parent_issuance_id'] = issuance['issuance_id']
        new_issuance['split_reason'] = 'è¤‡æ•°ç™ºè¡Œæ ¹æ‹ '
        
        if lb['amount']:
            new_issuance['issue_amount'] = lb['amount']
        else:
            unassigned_count = sum(1 for x in legal_bases if not x['amount'])
            if unassigned_count > 0 and original_amount:
                remaining = original_amount - total_extracted_amount
                new_issuance['issue_amount'] = remaining // unassigned_count
        
        split_issuances.append(new_issuance)
    
    return split_issuances


def process_announcement_file(file_path):
    """1ã¤ã®å‘Šç¤ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    file_name = os.path.basename(file_path)
    announcement_id = file_name.replace('.txt', '')
    
    # å‰²å¼•çŸ­æœŸå›½å‚µã¨æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®åˆ¤å®š
    has_waribiki = has_waribiki_tanki(content)
    has_seifu = has_government_short_term_bond(content)
    
    # æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µãªã—ï¼‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if has_seifu and not has_waribiki:
        print(f"  âš ï¸  æ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        return None
    
    # ä¸¡æ–¹å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
    if has_waribiki and has_seifu:
        print(f"  ğŸ“ å‰²å¼•çŸ­æœŸå›½å‚µ+æ”¿åºœçŸ­æœŸè¨¼åˆ¸ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µã®ã¿æŠ½å‡ºï¼‰")
    
    date_match = re.match(r'(\d{8})_', file_name)
    if date_match:
        date_str = date_match.group(1)
        kanpo_date = datetime.strptime(date_str, '%Y%m%d').date()
    else:
        kanpo_date = None
    
    announcement_number = extract_series_number(content)
    
    announcement_data = {
        'announcement_id': announcement_id,
        'kanpo_date': kanpo_date.isoformat() if kanpo_date else None,
        'announcement_number': announcement_number,
        'source_file': file_name,
        'created_at': datetime.now().isoformat(),
        'updated_at': datetime.now().isoformat()
    }
    
    legal_bases = extract_legal_bases(content)
    
    if legal_bases:
        # æ”¿åºœçŸ­æœŸè¨¼åˆ¸é–¢é€£ã®ç™ºè¡Œæ ¹æ‹ ã¯é™¤å¤–
        legal_bases_filtered = []
        for lb in legal_bases:
            # å€Ÿæ›å‚µï¼ˆç‰¹ä¼šæ³•46æ¡ï¼‰ã®ã¿ã‚’æ®‹ã™ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µã®å ´åˆï¼‰
            if lb['basis'] in ['å€Ÿæ›å‚µ', 'å‰å€’å‚µ', '4æ¡å‚µ', 'ç‰¹ä¾‹å‚µ', 'GXçµŒæ¸ˆç§»è¡Œå‚µ', 'å¾©èˆˆå‚µ']:
                legal_bases_filtered.append(lb)
        
        if legal_bases_filtered:
            basis_summary = ', '.join([f"{lb['basis']}({lb['category']})" for lb in legal_bases_filtered])
            print(f"  â†’ ç™ºè¡Œæ ¹æ‹ : {basis_summary}")
            legal_bases = legal_bases_filtered
    
    all_issuances = []
    if HAS_PARSERS:
        parser = TableParser()
        try:
            issuances = parser.extract_bond_info(content)
        except:
            issuances = [parser.extract_bond_info_from_single(content)]
        
        for issuance in issuances:
            if not issuance:
                continue
            
            issuance_date = get_issuance_attribute(issuance, 'issuance_date')
            maturity_date = get_issuance_attribute(issuance, 'maturity_date')
            payment_date = get_issuance_attribute(issuance, 'payment_date')
            issue_amount = get_issuance_attribute(issuance, 'issue_amount')
            issue_price = get_issuance_attribute(issuance, 'issue_price')
            interest_rate = get_issuance_attribute(issuance, 'interest_rate')
            series_number = get_issuance_attribute(issuance, 'series_number')
            bond_name = get_issuance_attribute(issuance, 'bond_name')
            
            days, months, years = calculate_maturity_period(issuance_date, maturity_date)
            bond_master_id = determine_bond_master_id(bond_name or '', days)
            
            issuance_date_str = parse_date_string(issuance_date)
            issuance_date_str = issuance_date_str.isoformat() if issuance_date_str else None
            
            maturity_date_str = parse_date_string(maturity_date)
            maturity_date_str = maturity_date_str.isoformat() if maturity_date_str else None
            
            payment_date_str = parse_date_string(payment_date)
            payment_date_str = payment_date_str.isoformat() if payment_date_str else None
            
            issuance_data = {
                'issuance_id': str(uuid.uuid4()),
                'announcement_id': announcement_id,
                'bond_master_id': bond_master_id,
                'series_number': series_number or extract_series_number(content),
                'issuance_date': issuance_date_str,
                'maturity_date': maturity_date_str,
                'payment_date': payment_date_str,
                'maturity_period_days': days,
                'maturity_period_months': months,
                'maturity_period_years': years,
                'issue_amount': issue_amount,
                'issue_price': issue_price,
                'interest_rate': interest_rate,
                'legal_basis': None,
                'legal_basis_full': None,
                'legal_basis_category': None,
                'issue_method': None,
                'is_split': False,
                'parent_issuance_id': None,
                'split_reason': None,
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }
            
            split_issuances = split_by_legal_basis(issuance_data, legal_bases, content)
            all_issuances.extend(split_issuances)
    else:
        print("  âš ï¸  ãƒ‘ãƒ¼ã‚µãƒ¼æœªä½¿ç”¨ï¼ˆç™ºè¡Œæ ¹æ‹ ã®ã¿è¨˜éŒ²ï¼‰")
    
    return {
        'announcement': announcement_data,
        'issuances': all_issuances
    }


def upload_to_bigquery(data_list):
    """BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    announcements = [d['announcement'] for d in data_list if d]
    if announcements:
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.announcements"
        errors = client.insert_rows_json(table_ref, announcements)
        if errors:
            print(f"âŒ announcementsæŠ•å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
        else:
            print(f"âœ… announcementsæŠ•å…¥æˆåŠŸ: {len(announcements)}ä»¶")
    
    all_issuances = []
    for d in data_list:
        if d and d['issuances']:
            all_issuances.extend(d['issuances'])
    
    if all_issuances:
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.bond_issuances"
        errors = client.insert_rows_json(table_ref, all_issuances)
        if errors:
            print(f"âŒ bond_issuancesæŠ•å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
        else:
            print(f"âœ… bond_issuancesæŠ•å…¥æˆåŠŸ: {len(all_issuances)}ä»¶")
    
    return len(all_issuances)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ20251024ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰")
    print("=" * 60)
    print()
    
    if not HAS_PARSERS:
        print("âš ï¸  è­¦å‘Š: parsersãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("âš ï¸  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ:", project_root)
        print("âš ï¸  ç™ºè¡Œæ ¹æ‹ æ³•ä»¤ã®ã¿ã‚’å‡¦ç†ã—ã¾ã™\n")
    
    files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.txt')])
    print(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}ä»¶\n")
    
    data_list = []
    skip_count = 0
    mixed_count = 0
    error_count = 0
    
    for i, file_name in enumerate(files, 1):
        file_path = os.path.join(DATA_DIR, file_name)
        print(f"[{i}/{len(files)}] {file_name}")
        
        try:
            result = process_announcement_file(file_path)
            if result:
                data_list.append(result)
                if result['issuances']:
                    print(f"  âœ… ç™ºè¡Œä»¶æ•°: {len(result['issuances'])}ä»¶")
                    # æ··åœ¨å‘Šç¤ºã®ã‚«ã‚¦ãƒ³ãƒˆ
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if has_waribiki_tanki(content) and has_government_short_term_bond(content):
                            mixed_count += 1
            else:
                skip_count += 1
        except Exception as e:
            error_count += 1
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\n=== å‡¦ç†å®Œäº† ===")
    print(f"æˆåŠŸ: {len(data_list)}ä»¶")
    print(f"  ã†ã¡æ··åœ¨å‘Šç¤º: {mixed_count}ä»¶ï¼ˆå‰²å¼•çŸ­æœŸå›½å‚µ+æ”¿åºœçŸ­æœŸè¨¼åˆ¸ï¼‰")
    print(f"ã‚¹ã‚­ãƒƒãƒ—: {skip_count}ä»¶ï¼ˆæ”¿åºœçŸ­æœŸè¨¼åˆ¸ã®ã¿ï¼‰")
    print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    
    if data_list:
        print("\n=== BigQueryæŠ•å…¥é–‹å§‹ ===")
        total_issuances = upload_to_bigquery(data_list)
        print(f"\nç·ç™ºè¡Œä»¶æ•°: {total_issuances}ä»¶")
        
        print("\n=== ç™ºè¡Œæ ¹æ‹ åˆ¥é›†è¨ˆ ===")
        legal_basis_summary = {}
        for d in data_list:
            if d and d['issuances']:
                for issuance in d['issuances']:
                    category = issuance.get('legal_basis_category')
                    if category:
                        if category not in legal_basis_summary:
                            legal_basis_summary[category] = {'count': 0, 'amount': 0}
                        legal_basis_summary[category]['count'] += 1
                        legal_basis_summary[category]['amount'] += issuance.get('issue_amount', 0) or 0
        
        for category in sorted(legal_basis_summary.keys()):
            data = legal_basis_summary[category]
            print(f"{category}: {data['count']}ä»¶, {data['amount'] / 100000000:,.0f}å„„å††")


if __name__ == "__main__":
    main()